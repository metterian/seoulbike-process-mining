{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë”°ë¦‰ì´ ì¸ìŠ¤íƒ€ê·¸ë¨ 300ê²Œì‹œê¸€ (ìµœì‹ ìˆœ) ì¶”ì¶œ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import csv\n",
    "with open('total.csv', 'r',encoding ='utf-8-sig') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    text = [] \n",
    "    for row in reader:\n",
    "     \n",
    "        text.append(row['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csvLoader(filename,colname):\n",
    "    with open(filename, 'r',encoding ='utf-8-sig') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        text = [] \n",
    "        for row in reader:\n",
    "     \n",
    "            text.append(row[colname])\n",
    "    return text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1258"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### í˜•íƒœì†Œ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_tag = []\n",
    "for sentence in text:\n",
    "    morph = twitter.pos(sentence)\n",
    "    sentences_tag.append(morph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### í˜•ìš©ì‚¬ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ì¢‹ì€', 25), ('ìˆëŠ”', 22), ('ì¢‹ë‹¤', 19), ('ì¢‹ì•„', 19), ('ì—†ëŠ”', 15), ('ì¢‹ì•„í•˜ëŠ”', 15), ('ì¢‹ì•„ìš”', 15), ('ê°™ì€', 14), ('ì…ë‹ˆë‹¤', 13), ('ì‹œì›í•œ', 13), ('ë§ì€', 13), ('ë”ìš´', 10), ('ê°™ë‹¤', 10), ('ê°™ì•„ì„œ', 10), ('ë¥ê³ ', 10), ('ì‹ ë‚˜ê²Œ', 9), ('ìˆì–´', 9), ('ì¢‹', 8), ('ë”ì›Œì„œ', 8), ('ë¥ë‹¤', 8)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1258"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_list = []\n",
    "for sentence1 in sentences_tag:\n",
    "    for word, tag in sentence1:\n",
    "        if tag in ['Adjective']:\n",
    "            adj_list.append(word)\n",
    "len(adj_list)\n",
    "counts_adj = Counter(adj_list)\n",
    "print(counts_adj.most_common(20))\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ëª…ì‚¬ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ë”°ë¦‰', 374), ('ìì „ê±°', 233), ('í•œê°•', 176), ('ì˜¤ëŠ˜', 130), ('íƒ€ê³ ', 125), ('ìš´ë™', 107), ('ì§‘', 80), ('ë”°ë¦‰ì´', 80), ('ë‚˜', 72), ('ì‹œê°„', 70), ('ë‚ ', 65), ('ê²ƒ', 63), ('ë‚´', 59), ('ì„œìš¸', 59), ('ë¹„', 59), ('ë”', 57), ('ë¼ì´ë”©', 55), ('ë•Œ', 51), ('ë¦‰', 46), ('ì¼ìƒ', 45), ('ë§Œ', 45), ('ë‚ ì”¨', 44), ('í‡´ê·¼', 44), ('ì´', 42), ('ê¸¸', 41), ('ì‚¬ëŒ', 39), ('ê³µì›', 39), ('ë°¤', 38), ('í›„', 38), ('ì§„ì§œ', 37)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3478"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_list = []\n",
    "for sentence1 in sentences_tag:\n",
    "    for word, tag in sentence1:\n",
    "        if tag in ['Noun']:\n",
    "            noun_list.append(word)\n",
    "counts = Counter(noun_list)\n",
    "print(counts.most_common(30))\n",
    "len(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#ë”°ë¦‰ì´', 861), ('#í•œê°•', 232), ('#ìì „ê±°', 186), ('#ì¼ìƒ', 107), ('#ìš´ë™', 86), ('#ì„œìš¸', 64), ('#ë¼ì´ë”©', 52), ('#daily', 47), ('#seoul', 39), ('#ì¢‹ì•„ìš”', 35), ('#í•œê°•ê³µì›', 35), ('#ë°ì¼ë¦¬', 33), ('#ìš´ë™í•˜ëŠ”ì—¬ì', 31), ('#ì†Œí†µ', 30), ('#í‡´ê·¼ê¸¸', 30), ('#ë§íŒ”', 30), ('#ì•¼ê²½', 28), ('#hanriver', 26), ('#selfie', 25), ('#ì¼ìƒìŠ¤íƒ€ê·¸ë¨', 24), ('#ë‹¤ì´ì–´íŠ¸', 24), ('#ìš´ë™ìŠ¤íƒ€ê·¸ë¨', 22), ('#korea', 22), ('#ì—¬ì˜ë„', 21), ('#ì—¬ì˜ë„í•œê°•ê³µì›', 20), ('#í•œê°•ë¼ì´ë”©', 20), ('#ì„ íŒ”', 19), ('#íŒ”ë¡œìš°', 18), ('#í‡´ê·¼', 18), ('#ìì „ê±°ê·¸ë¨', 18), ('#ì‚°ì±…', 16), ('#ì—¬ë¦„', 16), ('#ì£¼ë§', 15), ('#ì…€ìŠ¤íƒ€ê·¸ë¨', 15), ('#ë°ì´íŠ¸', 15), ('#ìƒˆë²½', 15), ('#ëšì„¬ìœ ì›ì§€', 15), ('#ì…€ì¹´', 14), ('#ì…€í”¼', 14), ('#running', 14), ('#bicycle', 14), ('#diet', 14), ('#ì¢‹ì•„ìš”ë°˜ì‚¬', 13), ('#ìì „ê±°ë¼ì´ë”©', 13), ('#ìì „ê±°íƒ€ê¸°', 12), ('#ì¤‘ë‘ì²œ', 12), ('#f4f', 12), ('#workout', 12), ('#ì„œìš¸ìì „ê±°', 12), ('#ì ì‹¤', 11)]\n"
     ]
    }
   ],
   "source": [
    "#í•´ì‰¬íƒœê·¸ ì¶”ì¶œ\n",
    "hashtag_list=[]\n",
    "for compo in sentences_tag:\n",
    "    for word, tag in compo:\n",
    "        if tag in ['Hashtag']:\n",
    "            hashtag_list.append(word)\n",
    "count_hash = Counter(hashtag_list)\n",
    "print (count_hash.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'customized konlpy'\n",
    "#### ìˆ˜ì •ëœ í˜•íƒœë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë¶„ì„ì„ ì§„í–‰í•¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckonlpy.tag import Postprocessor\n",
    "from ckonlpy.tag import Twitter\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "ctwitter = Twitter()\n",
    "ctwitter.add_dictionary(['ë”°ë¦‰ì´','ìì¶œ',\n",
    "                       'ìì „ê±°ê·¸ë¨','ëŸ½ìŠ¤íƒ€ê·¸ë¨',\n",
    "                       'ì¸ìŠ¤íƒ€ê·¸ë¨','daily','ë°ì¼ë¦¬'], 'Noun')\n",
    "ctwitter.add_dictionary('ê¹Œì§€', 'Josa')\n",
    "ctwitter.add_dictionary('ë ˆì•Œ', 'Adjective')\n",
    "ctwitter.add_dictionary('íƒ€ê³ ', 'Verb')\n",
    "stopwords = {'ê¹Œì§€','\\n','\\n\\n'}\n",
    "postprocessor = Postprocessor(ctwitter, stopwords = stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('total.csv', 'r',encoding ='utf-8-sig') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    c_text = [] \n",
    "    for row in reader:\n",
    "     \n",
    "        c_text.append(row['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_tag = []\n",
    "for sentence in c_text:\n",
    "    #post_data = postprocessor.pos(sentence)\n",
    "    morph = postprocessor.pos(sentence)\n",
    "    sentences_tag.append(morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ì¢‹', 32), ('ì¢‹ì€', 28), ('ìˆëŠ”', 27), ('ìˆ', 27), ('ì¢‹ë‹¤', 26), ('ì¢‹ì•„', 22), ('ì—†ëŠ”', 17), ('ê°™ì€', 15), ('ì…ë‹ˆë‹¤', 14), ('ë§ì€', 13), ('í˜ë“¤', 11), ('ê°™ë‹¤', 11), ('ë”ìš´', 10), ('ìˆì–´', 10), ('ê°™ì•„ì„œ', 10), ('ë¥ê³ ', 10), ('ì‹ ë‚˜ê²Œ', 9), ('ë¥ë‹¤', 9), ('ì¢‹ì•„ìš”', 9), ('ì—†ë‹¤', 9)]\n"
     ]
    }
   ],
   "source": [
    "adj_list = []\n",
    "for sentence1 in sentences_tag:\n",
    "    for word, tag in sentence1:\n",
    "        if tag in ['Adjective']:\n",
    "            adj_list.append(word)\n",
    "adj_counts = Counter(adj_list)\n",
    "print(adj_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ë”°ë¦‰ì´', 1541), ('í•œê°•', 608), ('ìì „ê±°', 576), ('ìš´ë™', 273), ('ê·¸ë¨', 242), ('íƒ€ê³ ', 214), ('ì˜¤ëŠ˜', 205), ('ì„œìš¸', 199), ('ì¼ìƒ', 197), ('ìŠ¤íƒ€', 189), ('ë¼ì´ë”©', 172), ('ê³µì›', 153), ('ì‹œê°„', 113), ('ì§‘', 98), ('ë‚˜', 85), ('ë‹¤ì´ì–´íŠ¸', 85), ('ì—¬ì˜ë„', 82), ('ë‚´', 78), ('í‡´ê·¼', 75), ('ë‚ ', 70), ('ë§íŒ”', 70), ('daily', 70), ('ê¸¸', 67), ('ê²ƒ', 66), ('ë‚ ì”¨', 65), ('ë‹¤ê°€', 65), ('ì‚¬ì§„', 63), ('ë”', 63), ('ì†Œí†µ', 62), ('ì´', 61), ('ë§›ì§‘', 61), ('ë°ì¼ë¦¬', 61), ('ë¹„', 58), ('ë°ì´íŠ¸', 56), ('ì•¼ê²½', 56), ('ë•Œ', 55), ('ìƒê°', 55), ('ì €ë…', 54), ('ì¹´í˜', 53), ('í•˜ëŠ˜', 53), ('í‡´ê·¼ê¸¸', 53), ('ì ì‹¤', 53), ('í–‰ë³µ', 53), ('ì–´ì„œ', 52), ('ìƒˆë²½', 52), ('ê³ ', 51), ('ë°¤', 51), ('ì—¬ë¦„', 51), ('ì¤‘', 50), ('ì„ íŒ”', 50), ('ëšì„¬', 50), ('ì£¼ë§', 49), ('íƒ€', 49), ('ì˜¤ëœë§Œ', 49), ('ë‹¤ë¦¬', 45), ('í•˜ë£¨', 45), ('ì‹œì›', 45), ('ì—¬ì', 44), ('ì‚¬ëŒ', 44), ('ì–´ìš”', 43), ('í›„', 43), ('ì§„ì§œ', 43), ('ë™í•˜', 41), ('ìš”', 41), ('ì‚°ì±…', 41), ('ì–´ì œ', 40), ('ìˆ˜', 39), ('ë‹¤ì‹œ', 39), ('ìš°ë¦¬', 38), ('ë°˜í¬', 37), ('ê¸°ë¶„', 37), ('ë©´ì„œ', 37), ('ë¼ë©´', 36), ('ì„œìš¸ì‹œ', 36), ('ë°”ëŒ', 35), ('ì—¬í–‰', 35), ('ê¸°ë¡', 35), ('ê·¸', 34), ('ê±°', 34), ('ë˜', 33), ('ëŸ¬ë‹', 33), ('ì•ˆ', 33), ('ì–¸ë‹ˆ', 33), ('ë§', 33), ('íœ´ê°€', 33), ('ë•€', 32), ('ì•„ì¹¨', 32), ('ê¸°', 32), ('ê°„', 31), ('ìš”ì¦˜', 31), ('í•˜ë‚˜', 30), ('íŒ”ë¡œìš°', 30), ('ë‹¬ë¦¬', 30), ('ì™œ', 29), ('ê·¸ëƒ¥', 29), ('íë§', 29), ('ì €', 29), ('ì¹œêµ¬', 28), ('ê°ì„±', 28), ('ì‹œì‘', 28)]\n"
     ]
    }
   ],
   "source": [
    "noun_list = []\n",
    "for sentence11 in sentences_tag:\n",
    "    for word, tag in sentence11:\n",
    "        if tag in ['Noun']:\n",
    "            noun_list.append(word)\n",
    "counts = Counter(noun_list)\n",
    "print(counts.most_common(100))\n",
    "tags = counts.most_common(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ã…‹ã…‹', 38), ('ã…‹ã…‹ã…‹', 27), ('ã…ã…', 24), ('ã… ã… ', 19), ('ã…‹ã…‹ã…‹ã…‹', 19), ('ã…‹', 17), ('ã…ã…ã…', 12), ('ã…œã…œ', 12), ('ã…‹ã…‹ã…‹ã…‹ã…‹', 10), ('ã… ', 9), ('ã…¡', 8), ('ã…œ', 7), ('ã…', 6), ('ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹', 5), ('ã„¹', 4), ('ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹', 4), ('ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹', 4), ('ã„±ã„±', 3), ('ã…ã…ã…ã…', 2), ('ã… ã…œ', 2)]\n"
     ]
    }
   ],
   "source": [
    "#korean particle ì¶”ì¶œ ex) 'ã…‹ã…‹', 'ã…ã…'\n",
    "kr_particle= []\n",
    "for sentence in sentences_tag:\n",
    "    for word, tag in sentence:\n",
    "        if tag in ['KoreanParticle']:\n",
    "            kr_particle.append(word)\n",
    "kr_counts = Counter(kr_particle)\n",
    "print(kr_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3141"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#ìì¶œ', 'Hashtag'), ('#ëŸ½', 'Hashtag')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "ctwitter = Twitter()\n",
    "from ckonlpy.tag import Postprocessor\n",
    "passtags = {'Foreign'}\n",
    "postprocessor = Postprocessor(ctwitter, passtags = passtags)\n",
    "postprocessor.pos('#ì¼ìƒ#ì¸ìŠ¤íƒ€ê·¸ë¨#ìì¶œ#ìš´ë™ìŠ¤íƒ€ê·¸ë¨#ëŸ½ìŠ¤íƒ€ê·¸ë¨ ì„œìš¸ê¹Œì§€')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ì´ëª¨í‹°ì½˜ë§Œ ì¶”ì¶œí•´ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ğŸš²', 147), ('â €', 64), ('ğŸš´ğŸ»\\u200dâ™€ï¸', 40), ('â¤ï¸', 27), ('ğŸ˜­', 21), ('ğŸ’•', 21), ('ğŸš´\\u200dâ™€ï¸', 21), ('â€¢', 21), ('ğŸ‘', 17), ('ğŸ’¦', 16), ('ğŸš´\\u200dâ™‚ï¸', 15), ('ğŸ¤£', 15), ('ğŸ˜‚', 14), ('â¤', 14), ('â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €', 14), ('ğŸ˜Š', 13), ('ğŸ¤”', 12), ('ğŸ’™', 12), ('ğŸ’š', 11), ('ğŸ¥°', 11), ('ğŸ˜†', 10), ('âœ”', 10), ('ğŸ”¥', 9), ('ğŸš²ğŸš²', 9), ('â €â €â €', 9), ('â™¡', 8), ('ã†', 8), ('â €â €â €â €â €â €', 8), ('ã€°ï¸', 8), ('ğŸš²ğŸš²ğŸš²', 7), ('ğŸ¶', 7), ('ğŸ˜', 7), ('ğŸ‘€', 7), ('ğŸ˜', 7), ('ğŸ˜±', 7), ('ğŸ¤­', 7), ('ğŸŒ™', 7), ('ğŸš´ğŸ»\\u200dâ™‚ï¸', 6), ('ğŸ™', 6), ('ğŸ˜', 6), ('âœ¨', 6), ('â˜ºï¸', 6), ('â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €', 6), ('ğŸ¥º', 6), ('ğŸ˜‹', 6), ('ğŸŒ‰', 6), ('ğŸ˜', 6), ('ğŸ™Š', 6), ('ğŸ¤—', 6), ('ğŸ˜³', 6), ('â€˜', 6), ('ğŸ‘ğŸ»', 5), ('ğŸ˜¤', 5), ('ğŸ¥µ', 5), ('â£ï¸', 5), ('ğŸ¤ª', 5), ('ğŸ’›', 5), ('â €â €', 5), ('ğŸ˜¢', 5), ('ğŸ™„', 5), ('ğŸ’œ', 5), ('â˜”ï¸', 5), ('ğŸ˜', 5), ('ğŸ', 4), ('â™¥ï¸', 4), ('ğŸ’§', 4), ('ğŸ‘ğŸ»ğŸ‘ğŸ»', 4), ('ğŸ˜£', 4), ('ğŸ™ğŸ»', 4), ('âœ”ï¸', 4), ('ğŸ‘­', 4), ('â™¥', 4), ('ğŸŒƒ', 4), ('ğŸ¤˜ğŸ»', 4), ('ğŸ¤¦\\u200dâ™€ï¸', 4), ('ğŸ˜¥', 4), ('â €â €â €â €â €', 4), ('ğŸ“¸', 4), ('ğŸ–¤', 4), ('ğŸ™ˆ', 4), ('ğŸŒŸ', 4), ('ğŸ˜‚ğŸ˜‚', 4), ('ğŸ˜š', 4), ('ğŸ¤ŸğŸ»', 4), ('â°', 4), ('ğŸš´', 4), ('â¡ï¸', 4), ('ğŸ˜‡', 3), ('ğŸ˜µ', 3), ('âœŒğŸ»', 3), ('ãƒ»ãƒ»ãƒ»', 3), ('ğŸ—', 3), ('ğŸ¤©', 3), ('à·†', 3), ('ğŸ˜…', 3), ('ğŸš´\\u200dâ™€ï¸ğŸš´\\u200dâ™‚ï¸', 3), ('ğŸ˜‰', 3), ('ğŸ‡°ğŸ‡·', 3), ('â›…ï¸', 3), ('ğŸ™‚', 3), ('ğŸš¶ğŸ»\\u200dâ™‚ï¸', 3), ('ğŸ±', 3), ('â€¼ï¸', 3), ('ğŸ‘‰ğŸ»', 3), ('ğŸŒ¿', 3), ('ğŸš´ğŸ¼\\u200dâ™€ï¸', 3), ('Ğ—', 3), ('ğŸš´\\u200dâ™‚ï¸ğŸš´\\u200dâ™€ï¸', 3), ('ğŸ™ƒ', 3), ('ğŸ™‹ğŸ»\\u200dâ™€ï¸', 3), ('ì—', 3), ('ğŸ–', 3), ('ğŸš´ğŸ½\\u200dâ™‚ï¸ğŸš´ğŸ½\\u200dâ™‚ï¸', 2), ('ğŸœ', 2), ('ğŸ“©', 2), ('ğŸ™', 2), ('ğŸ¾', 2), ('ì˜', 2), ('ğŸƒ', 2), ('ğŸŒ»', 2), ('ğŸ˜¬', 2), ('â¤ï¸ğŸ’›ğŸ§¡ğŸ’™', 2), ('ğŸ·', 2), ('ğŸº', 2), ('ğŸ‘ğŸ‘ğŸ‘', 2), ('ğŸ™†\\u200dâ™€ï¸', 2), ('ğŸ‡ªğŸ‡¸', 2), ('â€»', 2), ('ğŸ“ŒğŸ“ŒğŸ“Œ', 2), ('ğŸƒ\\u200dâ™€ï¸ğŸƒ\\u200dâ™‚ï¸', 2), ('ğŸ§\\u200dâ™‚ï¸', 2), ('â˜€ï¸', 2), ('ğŸ˜Œ', 2), ('ì¯¤', 2), ('ğŸ¤ªğŸ¤ª', 2), ('ğŸ˜‘', 2), ('ğŸ‘ğŸ»', 2), ('ğŸ˜¡', 2), ('â€¢Ì…', 2), ('ğŸŒƒğŸ’™', 2), ('ğŸ‘', 2), ('ğŸ’“', 2), ('ğŸ˜—', 2), ('ğŸ¤”ğŸ¤”', 2), ('ğŸ˜—ğŸ’•', 2), ('ğŸ˜¶', 2), ('âœ…', 2), ('â˜†', 2), ('ğŸ˜˜', 2), ('ğŸ™ŒğŸ»', 2), ('â¢', 2), ('à¸…', 2), ('ğŸ˜­ğŸ˜­', 2), ('ğŸš´ğŸ»\\u200dâ™€ï¸ğŸ’¨', 2), ('ğŸ˜“', 2), ('ğŸƒ\\u200dâ™‚ï¸', 2), ('ğŸš²ğŸ’•', 2), ('âœŒ', 2), ('ğŸ‘‰', 2), ('ê¶Œ', 2), ('â˜€', 2), ('ğŸ¤£ğŸ¤£ğŸ¤£', 2), ('ğŸ˜ğŸ˜ğŸ˜', 2), ('âš¡', 2), ('ğŸšµğŸ»\\u200dâ™€ï¸', 2), ('ğŸ´', 2), ('ğŸ›´', 2), ('ğŸ™', 2), ('ğŸ–', 2), ('ğŸ¶', 2), ('á†¢', 2), ('ğŸ‘', 2), ('Í¡áµ”', 2), ('ğŸ˜¥ğŸ˜¥', 2), ('ğŸƒğŸƒ', 2), ('ğŸ¢ğŸ¢', 2), ('âš ï¸', 2), ('ğŸš²ğŸš²ğŸš²ğŸš²ğŸš²', 2), ('â½', 2), ('ğŸ¤£ğŸ¤£', 2), ('ğŸ’—', 2), ('ï¼¾', 2), ('ğŸ‰', 1), ('ğŸŒ¹ğŸŒ¹', 1), ('ğŸŠğŸ»\\u200dâ™‚ï¸', 1), ('ğŸ‡ª', 1), ('ğŸ¤­ğŸ¤­', 1), ('â˜ºï¸â˜ºï¸', 1), ('ğŸ¤ªğŸ¤ªğŸ¤ªğŸ¤ªğŸ¤ª', 1), ('âœŒğŸ»ğŸ–ğŸ»', 1), ('âœŒğŸ»âœŒğŸ»', 1), ('ğŸ‘ğŸ»ğŸ’•', 1), ('ğŸ˜©ğŸ’¦ğŸ’¦', 1), ('â™ª', 1), ('ğŸš²ğŸ‘‹ğŸ»', 1), ('ğŸš´ğŸ»\\u200dâ™‚ï¸ğŸš´ğŸ»\\u200dâ™‚ï¸', 1), ('íƒ”ëŠ”ë°', 1), ('ğŸ¥°ğŸ¥°ğŸ¥°ğŸŠğŸŠ', 1), ('ğŸŠ', 1), ('ğŸ™Œ', 1)]\n"
     ]
    }
   ],
   "source": [
    "emotion_list = []\n",
    "for emo in sentences_tag:\n",
    "    for word, tag in emo:\n",
    "        if tag in ['Foreign']:\n",
    "            emotion_list.append(word)\n",
    "emo_counts = Counter(emotion_list)\n",
    "print(emo_counts.most_common(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#ë”°ë¦‰ì´', 779), ('#í•œê°•', 211), ('#ìì „ê±°', 169), ('#ì¼ìƒ', 100), ('#ìš´ë™', 79), ('#ì„œìš¸', 56), ('#ë¼ì´ë”©', 46), ('#daily', 44), ('#seoul', 38), ('#ì¢‹ì•„ìš”', 32), ('#ë°ì¼ë¦¬', 31), ('#í•œê°•ê³µì›', 30), ('#ë§íŒ”', 29), ('#ìš´ë™í•˜ëŠ”ì—¬ì', 28), ('#í‡´ê·¼ê¸¸', 28), ('#ì†Œí†µ', 26), ('#hanriver', 26), ('#ë‹¤ì´ì–´íŠ¸', 23), ('#selfie', 23), ('#ì—¬ì˜ë„', 21), ('#ì¼ìƒìŠ¤íƒ€ê·¸ë¨', 21), ('#ì•¼ê²½', 21), ('#korea', 21), ('#ìš´ë™ìŠ¤íƒ€ê·¸ë¨', 19), ('#ì„ íŒ”', 18), ('#í•œê°•ë¼ì´ë”©', 18), ('#ì—¬ì˜ë„í•œê°•ê³µì›', 17), ('#ìì „ê±°ê·¸ë¨', 17), ('#íŒ”ë¡œìš°', 16), ('#í‡´ê·¼', 16)]\n"
     ]
    }
   ],
   "source": [
    "hashtag_list = []\n",
    "for emo in sentences_tag:\n",
    "    for word, tag in emo:\n",
    "        if tag in ['Hashtag']:\n",
    "            hashtag_list.append(word)\n",
    "counts = Counter(hashtag_list)\n",
    "print(counts.most_common(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(emo_counts, orient='index').reset_index()\n",
    "df = df.rename(columns={'index':'noun', 0:'count'})\n",
    "df=df.sort_values(by=['count'], axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('insta_emotion.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
